{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13bd9197",
   "metadata": {},
   "source": [
    "# <center> Customer Churn Prediction Using LSTM üë®üèª‚Äçüíª</center>\n",
    "\n",
    "## <center>[Msc. Diego Hurtado](https://www.linkedin.com/in/diegohurtadoo/)</center>\n",
    "\n",
    "<img src=\"https://media.tenor.com/b2kdbWrrNZQAAAAC/going-in-the-portal-tom-holland.gif\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505a87b",
   "metadata": {},
   "source": [
    "# 1 <a id='1'>Introduction</a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150e9b27",
   "metadata": {},
   "source": [
    "Telecom or telecommunications industry is one of the fastest-growing and rapidly evolving industries. With the increasing competition, it has become more important than ever for telecom companies to retain their customers. In this context, predicting customer churn, i.e., the likelihood of a customer leaving a company, has become a crucial task for telecom companies.\n",
    "\n",
    "This Telco Churn Prediction dataset, provided by IBM, contains a sample of customer data with attributes such as customer services, account information, and demographics. The dataset also includes a binary label indicating whether the customer has churned or not. The goal of this dataset is to predict whether a customer is likely to churn or not based on their profile and services subscribed.\n",
    "\n",
    "By analyzing the customer data and developing retention strategies, telecom companies can not only retain their customers but also acquire new customers by attracting customers from their competitors. In this regard, machine learning models can be used to predict churn and identify the most important features that contribute to customer churn. This can help telecom companies develop focused customer retention programs and improve their business performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096bfd51",
   "metadata": {},
   "source": [
    "customerID : Customer ID\n",
    "\n",
    "gender : Whether the customer is a male or a female\n",
    "\n",
    "SeniorCitizen : Whether the customer is a senior citizen or not (1, 0)\n",
    "\n",
    "Partner : Whether the customer has a partner or not (Yes, No)\n",
    "\n",
    "Dependents : Whether the customer has dependents or not (Yes, No)\n",
    "\n",
    "tenure : Number of months the customer has stayed with the company\n",
    "\n",
    "PhoneService : Whether the customer has a phone service or not (Yes, No)\n",
    "\n",
    "MultipleLines : Whether the customer has multiple lines or not (Yes, No, No phone service)\n",
    "\n",
    "InternetService : Customer‚Äôs internet service provider (DSL, Fiber optic, No)\n",
    "\n",
    "OnlineSecurity : Whether the customer has online security or not (Yes, No, No internet service)\n",
    "\n",
    "OnlineBackup : Whether the customer has online backup or not (Yes, No, No internet service)\n",
    "\n",
    "DeviceProtection : Whether the customer has device protection or not (Yes, No, No internet service)\n",
    "\n",
    "TechSupport : Whether the customer has tech support or not (Yes, No, No internet service)\n",
    "\n",
    "StreamingTV : Whether the customer has streaming TV or not (Yes, No, No internet service)\n",
    "\n",
    "StreamingMovies : Whether the customer has streaming movies or not (Yes, No, No internet service)\n",
    "\n",
    "Contract : The contract term of the customer (Month-to-month, One year, Two year)\n",
    "\n",
    "PaperlessBilling : Whether the customer has paperless billing or not (Yes, No)\n",
    "\n",
    "PaymentMethod : The customer‚Äôs payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n",
    "\n",
    "MonthlyCharges : The amount charged to the customer monthly\n",
    "\n",
    "TotalCharges : The total amount charged to the customer\n",
    "\n",
    "Churn : Whether the customer churned or not (Yes or No)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f4e388",
   "metadata": {},
   "source": [
    "# Table of Contents Diego üë®üèª‚Äçüíª<a id='0.1'></a>\n",
    "\n",
    "* [Introduction](#1)\n",
    "* [Data Modeling ](#2)\n",
    "    * [Import Packages](#2.1)\n",
    "    * [Custom Classes](#2.2)\n",
    "    * [Data Preprocessing](#2.3)\n",
    "        * [Data Reading](#2.3.1)\n",
    "        * [Data Cleaning](#2.3.2)\n",
    "        * [feature engineering](#2.3.3)\n",
    "        * [Basic EDA](#2.3.4)\n",
    "    * [Data Preprocessing](#2.4)\n",
    "        * [LSTMClassifier](#2.4.1)\n",
    "        * [Data Cleaning](#2.4.2)\n",
    "        * [Evaluate](#2.4.3)\n",
    "        * [LSTM Prediction](#2.4.4)\n",
    "        * [Feature importance](#2.4.5)\n",
    "        * [Save LSTM Model](#2.4.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c145c82a",
   "metadata": {},
   "source": [
    "# 2 <a id='2'> Data Modeling üìö</a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c102a",
   "metadata": {},
   "source": [
    "# 2.1 <a id='2.1'>Import Packagesüìö</a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca520420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import sqrt\n",
    "import sys\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import category_encoders as ce\n",
    "# Evaluation Metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve, f1_score, recall_score\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO \n",
    "import IPython, graphviz\n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import re\n",
    "\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
    "import json\n",
    "\n",
    "plt.style.use('seaborn-dark-palette')\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 8]\n",
    "\n",
    "width = 1000\n",
    "height = 750\n",
    "\n",
    "bg_color = '#FFFFFF'\n",
    "paper_bg = '#FFFFFF'\n",
    "\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "color_charts = '#2baae2'\n",
    "plt.style.use('ggplot')\n",
    "color = '#16171f'\n",
    "plt.rcParams['text.color'] = color\n",
    "plt.rcParams['axes.labelcolor'] = color\n",
    "plt.rcParams['xtick.color'] = color\n",
    "plt.rcParams['ytick.color'] = color\n",
    "\n",
    "plt.rcParams.update({'text.color' : color,\n",
    "                             'axes.labelcolor' : color})\n",
    "\n",
    "plt.rcParams.update({'font.size': 17})\n",
    "plt.rc('font', size=17)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score, roc_curve, f1_score, recall_score\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import iplot, init_notebook_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d9a13",
   "metadata": {},
   "source": [
    "# 2.2 <a id='2.1'>Custom Classesüîç</a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pipeline_churn_prediction():\n",
    "    def __init__(self):\n",
    "        self.project = 'churn prediction '\n",
    "        \n",
    "    def get_percent_missing(self, df):\n",
    "        percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "        missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                             'percent_missing': percent_missing})\n",
    "        missing_value_df.sort_values('percent_missing', inplace=True)\n",
    "        percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "        \n",
    "        print('Percentage of Missing Values: ')\n",
    "\n",
    "        return percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_features(df):\n",
    "    \"\"\"\n",
    "    Encodes the specified categorical features of the given dataframe using Label Encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe.\n",
    "    features (list): List of categorical features to be encoded.\n",
    "    \n",
    "    Returns:\n",
    "    encoded_df (pandas.DataFrame): The encoded dataframe with the same columns as the input dataframe.\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "    encoded_df = df.copy()\n",
    "    \n",
    "    features = [i for i in list(df.columns) if i not in list(df.describe().columns)]\n",
    "    \n",
    "    print('Label Encoder Transformation')\n",
    "    for feature in features:\n",
    "        encoded_df[feature] = le.fit_transform(encoded_df[feature])\n",
    "        print(f'{feature}: {len(encoded_df[feature].unique())} unique value(s)')\n",
    "        print(f'Unique values: {list(encoded_df[feature].unique())}\\n')\n",
    "        \n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529f94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_counts(df):\n",
    "    \"\"\"\n",
    "    Returns the number of unique values and unique values for each feature in the given dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The input dataframe.\n",
    "\n",
    "    Returns:\n",
    "    unique_counts_df (pandas.DataFrame): A dataframe containing the feature names, the number of unique values, the unique values, and the data type of each feature.\n",
    "    \"\"\"\n",
    "    unique_counts = df.nunique()\n",
    "    unique_values = [df[column].unique() for column in df.columns]\n",
    "    data_types = [str(df[column].dtype) for column in df.columns]\n",
    "    unique_counts_df = pd.DataFrame({'feature': df.columns, 'unique_count': unique_counts, 'unique_values': unique_values, 'data_type': data_types})\n",
    "    return unique_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02773355",
   "metadata": {},
   "source": [
    "# 2.3 <a id='2.3'> Data Preprocessing üîç</a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564b96c",
   "metadata": {},
   "source": [
    "# 2.3.1 <a id='2.3.1'> Data Reading </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058169b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [16, 8]\n",
    "pipeline_churn_diego = pipeline_churn_prediction()\n",
    "\n",
    "df_train = pd.read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "id_sub = df_train.customerID\n",
    "df_train = df_train.drop(df_train.columns[0],axis=1)\n",
    "\n",
    "print(df_train.shape[0])\n",
    "print(len(df_train.columns.tolist()))\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the missing values \n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d618b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_churn_diego.get_percent_missing(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59408027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['TotalCharges', 'MonthlyCharges']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9631da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1994972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the string data\n",
    "df_train.describe(include='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef34c0b",
   "metadata": {},
   "source": [
    "# 2.3.2 <a id='2.3.2'> Data Cleaning </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6873c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['TotalCharges'] == ' ', 'TotalCharges'] = np.nan\n",
    "df_train['TotalCharges'] = df_train['TotalCharges'].astype(float)\n",
    "df_train.fillna(-99, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts(df_train.select_dtypes(include=['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8047ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the head of the data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef893d",
   "metadata": {},
   "source": [
    "# 2.3.3 <a id='2.3.3'> feature engineering </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4351df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['internet']= np.where(df_train.InternetService != 'No', 'Yes', 'No')\n",
    "\n",
    "df_train['num_services'] = (df_train[['PhoneService', 'OnlineSecurity',\n",
    "                                      'OnlineBackup', 'DeviceProtection', \n",
    "                                      'TechSupport', 'StreamingTV', \n",
    "                                      'StreamingMovies', 'internet']] == 'Yes').sum(axis=1)\n",
    "\n",
    "df_train['Engaged'] = (df_train['Contract'] != 'Month-to-month').astype(int)\n",
    "df_train['YandNotE'] = ((df_train['SeniorCitizen'] == 0) & (df_train['Engaged'] == 0)).astype(int)\n",
    "df_train['ElectCheck'] = ((df_train['PaymentMethod'].eq('Electronic check')) & (df_train['Engaged'] == 0)).astype(int)\n",
    "df_train['fiberopt'] = (df_train['InternetService'] != 'Fiber optic').astype(int)\n",
    "df_train['StreamNoInt'] = (df_train['StreamingTV'] != 'No internet service').astype(int)\n",
    "df_train['NoProt'] = ((df_train['OnlineBackup'] != 'No') | (df_train['DeviceProtection'] != 'No') | (df_train['TechSupport'] != 'No')).astype(int)\n",
    "\n",
    "services = ['PhoneService', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "df_train['TotalServices'] = df_train[services].eq('Yes').sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e79bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Churn'] = df_train['Churn'].replace('Yes', 1)\n",
    "df_train['Churn'] = df_train['Churn'].replace('No', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9838b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the colors and fonts\n",
    "colors = ['#E71D36', '#1F5673', '#F4D35E', '#EE964B']\n",
    "font = dict(family='Arial', size=10)\n",
    "\n",
    "#Label encoding Binary columns\n",
    "le = LabelEncoder()\n",
    "tmp_churn = df_train[df_train['Churn'] == 1]\n",
    "tmp_no_churn = df_train[df_train['Churn'] == 0]\n",
    "bi_cs = df_train.nunique()[df_train.nunique() == 2].keys()\n",
    "dat_rad = df_train[bi_cs]\n",
    "\n",
    "for cols in bi_cs :\n",
    "    tmp_churn[cols] = le.fit_transform(tmp_churn[cols])\n",
    "\n",
    "data_frame_x = tmp_churn[bi_cs].sum().reset_index()\n",
    "data_frame_x.columns  = [\"feature\",\"yes\"]\n",
    "data_frame_x[\"no\"]    = tmp_churn.shape[0]  - data_frame_x[\"yes\"]\n",
    "data_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n",
    "\n",
    "#count of 1's(yes)\n",
    "trace1 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(), \n",
    "                         theta = data_frame_x[\"feature\"].tolist(),\n",
    "                         fill  = \"toself\",name = \"Churn 1's\",\n",
    "                         mode = \"markers+lines\", visible=True,\n",
    "                         line=dict(color=colors[0], width=2),\n",
    "                         marker = dict(size = 8, color=colors[0])\n",
    "                        )\n",
    "\n",
    "#count of 0's(No)\n",
    "trace2 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n",
    "                         theta = data_frame_x[\"feature\"].tolist(),\n",
    "                         fill  = \"toself\",name = \"Churn 0's\",\n",
    "                         mode = \"markers+lines\", visible=True,\n",
    "                         line=dict(color=colors[1], width=2),\n",
    "                         marker = dict(size = 8, color=colors[1])\n",
    "                        ) \n",
    "\n",
    "for cols in bi_cs :\n",
    "    tmp_no_churn[cols] = le.fit_transform(tmp_no_churn[cols])\n",
    "\n",
    "data_frame_x = tmp_no_churn[bi_cs].sum().reset_index()\n",
    "data_frame_x.columns  = [\"feature\",\"yes\"]\n",
    "data_frame_x[\"no\"]    = tmp_no_churn.shape[0]  - data_frame_x[\"yes\"]\n",
    "data_frame_x  = data_frame_x[data_frame_x[\"feature\"] != \"Churn\"]\n",
    "\n",
    "#count of 1's(yes)\n",
    "trace3 = go.Scatterpolar(r = data_frame_x[\"yes\"].values.tolist(),\n",
    "                         theta = data_frame_x[\"feature\"].tolist(),\n",
    "                         fill  = \"toself\",name = \"NoChurn 1's\",\n",
    "                         mode = \"markers+lines\", visible=False,\n",
    "                         line=dict(color=colors[2], width=2),\n",
    "                         marker = dict(size = 8, color=colors[2])\n",
    "                        )\n",
    "\n",
    "#count of 0's(No)\n",
    "trace4 = go.Scatterpolar(r = data_frame_x[\"no\"].values.tolist(),\n",
    "                         theta = data_frame_x[\"feature\"].tolist(),\n",
    "                         fill  = \"toself\",name = \"NoChurn 0's\",\n",
    "                         mode = \"markers+lines\", visible=False,\n",
    "                         line=dict(color=colors[3], width=2),\n",
    "                         marker = dict(size = 8, color=colors[3])\n",
    "                        ) \n",
    "\n",
    "data = [trace1, trace2, trace3, trace4]\n",
    "\n",
    "updatemenus = list([\n",
    "    dict(active=0,\n",
    "         x=-0.15,\n",
    "         buttons=list([  \n",
    "            dict(\n",
    "                label = 'Churn Dist',\n",
    "                 method = 'update',\n",
    "                 args = [{'visible': [True, True, False, False]}, \n",
    "                     {'title': 'Customer Churn Binary Counting Distribution'}]),\n",
    "             \n",
    "             dict(\n",
    "                  label = 'No-Churn Dist',\n",
    "                 method = 'update',\n",
    "                 args = [{'visible': [False, False, True, True]},\n",
    "                     {'title': 'No Customer Churn Binary Counting Distribution'}]),\n",
    "\n",
    "        ]),\n",
    "    )\n",
    "])\n",
    "\n",
    "# update the layout\n",
    "layout = dict(\n",
    "    title='ScatterPolar Distribution of Churn and Non-Churn Customers (Select from Dropdown)', \n",
    "    showlegend=False,\n",
    "    updatemenus=updatemenus,\n",
    "    font=font,\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            showticklabels=True,\n",
    "            tickangle=0,\n",
    "            tickfont=dict(family='Arial', size=10),\n",
    "            ticksuffix=' customers',\n",
    "            ticklen=10,\n",
    "            range=[0, 1800]\n",
    "        ),\n",
    "        angularaxis=dict(\n",
    "            tickfont=dict(family='Arial', size=10),\n",
    "            ticks='outside',\n",
    "            tickcolor='#DDD',\n",
    "            ticklen=10\n",
    "        ),\n",
    "    ),\n",
    "    paper_bgcolor='rgb(240,240,240)',\n",
    "    plot_bgcolor='rgb(240,240,240)',\n",
    ")\n",
    "\n",
    "# create the figure and apply the layout\n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "# show the chart\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = label_encode_features(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3396f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(df_train.columns)\n",
    "categorical_features = []\n",
    "numerical_features = []\n",
    "for i in col:\n",
    "    if len(df_train[i].unique()) > 6:\n",
    "        numerical_features.append(i)\n",
    "    else:\n",
    "        categorical_features.append(i)\n",
    "\n",
    "print('Categorical Features :',*categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa28705",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numerical Features :',*numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a64bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "mms = MinMaxScaler() # Normalization\n",
    "ss = StandardScaler() # Standardization\n",
    "\n",
    "df_train['MonthlyCharges_Group'] = [int(i / 5) for i in df_train['MonthlyCharges']]\n",
    "df_train['TotalCharges_Group'] = [int(i / 500) for i in df_train['TotalCharges']]\n",
    "\n",
    "df_train.drop(columns = ['MonthlyCharges_Group','TotalCharges_Group'], inplace = True)\n",
    "\n",
    "df_train['tenure'] = mms.fit_transform(df_train[['tenure']])\n",
    "df_train['MonthlyCharges'] = mms.fit_transform(df_train[['MonthlyCharges']])\n",
    "df_train['TotalCharges'] = mms.fit_transform(df_train[['TotalCharges']])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d4a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191574fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into X and y\n",
    "X = df_train.drop('Churn', axis=1)\n",
    "y = df_train['Churn']\n",
    "\n",
    "# Fit a Random Forest Classifier model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the feature importance scores\n",
    "importance_scores = model.feature_importances_\n",
    "\n",
    "# Create a list of feature names and their importance scores\n",
    "feature_importance = list(zip(X.columns, importance_scores))\n",
    "\n",
    "# Sort the features by importance score in descending order\n",
    "feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "df_feature_importance = pd.DataFrame(feature_importance, columns=['feature', 'importance_score'])\n",
    "\n",
    "df_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfacff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart to show the feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set the x and y values\n",
    "x_values = [x[0] for x in feature_importance]\n",
    "y_values = [x[1] for x in feature_importance]\n",
    "\n",
    "# Plot the bar chart\n",
    "sns.barplot(x=x_values, y=y_values, ax=ax, color='b')\n",
    "\n",
    "# Set the x-axis labels to be rotated\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Feature Importance Scores')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Importance Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bcb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the top 10 most important features\n",
    "top_features = [feature for feature, score in feature_importance[:15]]\n",
    "\n",
    "# Create a new dataset with only the top features\n",
    "df_train = df_train[top_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af7cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.90\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = df_train.corr().abs()\n",
    "\n",
    "# Getting the upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove.' % (len(to_drop)))\n",
    "print(list(to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns = to_drop)\n",
    "print('Training shape: ', df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['churn'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481056",
   "metadata": {},
   "source": [
    "# 2.3.4 <a id='2.3.4'> Super Basic EDA </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_train)\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1dd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation and filter for high correlation\n",
    "corr = df_train.corr()\n",
    "high_corr = corr[abs(corr) > 0.6]\n",
    "\n",
    "# create heatmap with improved style\n",
    "sns.set(font_scale=1.2)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(high_corr, annot=True, annot_kws={\"size\": 8}, cmap='coolwarm', linewidths=.5, cbar=True, square=True, ax=ax)\n",
    "ax.set_title('High Correlation Features', fontsize=14)\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Features')\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pearsonr(df_train['MonthlyCharges'], df_train['TechSupport'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dbcb8",
   "metadata": {},
   "source": [
    "# 2.4 <a id='2.4'> Model üîç</a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e783535",
   "metadata": {},
   "source": [
    "# 2.4.1 <a id='2.4.1'> LSTMClassifier </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9eeae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the LSTM model outside the class\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, input_shape=input_shape, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the LSTMClassifier class\n",
    "class LSTMClassifier:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Class Created by: Diego Gustavo Hurtado Olivares\n",
    "    \n",
    "    LSTMClassifier is a class that provides an easy-to-use interface for training, evaluating, \n",
    "    and tuning Long Short-Term Memory (LSTM) models for binary classification tasks.\n",
    "    \n",
    "    This class includes methods for preprocessing data, building and training the LSTM model,\n",
    "    evaluating the model using various metrics, plotting the training history and ROC curve,\n",
    "    predicting new instances, saving and loading the model, and obtaining feature importances \n",
    "    using permutation importance. It also provides support for hyperparameter tuning, K-fold\n",
    "    cross-validation, and early stopping.\n",
    "    \n",
    "    Example usage:\n",
    "    --------------\n",
    "    \n",
    "    # Initialize the LSTMClassifier with your data\n",
    "    lstm_classifier = LSTMClassifier(data)\n",
    "    \n",
    "    # Preprocess the data\n",
    "    lstm_classifier.preprocess_data()\n",
    "    \n",
    "    # Build the LSTM model\n",
    "    lstm_classifier.build_model()\n",
    "    \n",
    "    # Train the model\n",
    "    history = lstm_classifier.train_model(epochs=50, batch_size=32)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    lstm_classifier.evaluate_model()\n",
    "    \n",
    "    # Plot the training history and ROC curve\n",
    "    lstm_classifier.plot_training_history(history)\n",
    "    lstm_classifier.plot_roc_curve()\n",
    "    \n",
    "    # Predict new instances\n",
    "    y_pred = lstm_classifier.predict(X_new)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        # Initialize the data path and variables to store the data, train and test sets, and model\n",
    "        # self.data_path = data_path\n",
    "        self.data = data\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.scaler = None\n",
    "        self.model = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        # Load the data from the specified path\n",
    "        self.data = pd.read_csv(self.data_path)\n",
    "        \n",
    "    def preprocess_data(self):\n",
    "        # Split the data into features (X) and target (y)\n",
    "        X = self.data.drop([\"churn\"], axis=1)\n",
    "        y = self.data[\"churn\"]\n",
    "        \n",
    "        # Split the data into train and test sets\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Standardize the features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Reshape the features to be 3D arrays suitable for input into an LSTM model\n",
    "        self.X_train = np.reshape(self.X_train, (self.X_train.shape[0], 1, self.X_train.shape[1]))\n",
    "        self.X_test = np.reshape(self.X_test, (self.X_test.shape[0], 1, self.X_test.shape[1]))\n",
    "        \n",
    "    def build_model(self):\n",
    "        # Create a sequential model with two LSTM layers, two dropout layers, and a dense output layer\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(64, input_shape=(1, self.X_train.shape[2]), return_sequences=True))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(32))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile the model with the binary crossentropy loss function, the Adam optimizer, and accuracy metrics\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    def early_stopping(self, patience=10, restore_best_weights=True):\n",
    "        return EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=restore_best_weights)\n",
    "    \n",
    "    def custom_metric(self, y_true, y_pred):\n",
    "        # Example: Implement the balanced accuracy metric\n",
    "        balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "        return balanced_accuracy\n",
    "    \n",
    "        \n",
    "    def evaluate_model(self, use_custom_metric=True):\n",
    "        y_pred_prob = self.model.predict(self.X_test)\n",
    "        y_pred_rounded = np.round(y_pred_prob)\n",
    "        y_pred = y_pred_rounded.astype(int).ravel()\n",
    "\n",
    "        # Evaluation metrics\n",
    "        confusion_mat = confusion_matrix(self.y_test, y_pred)\n",
    "        classification_re = classification_report(self.y_test, y_pred)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred)\n",
    "        recall = recall_score(self.y_test, y_pred)\n",
    "\n",
    "        # Print the evaluation metrics\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(confusion_mat)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_re)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"F1 Score:\", f1)\n",
    "        print(\"Recall:\", recall)\n",
    "\n",
    "        if use_custom_metric:\n",
    "            custom_metric_value = self.custom_metric(self.y_test, y_pred)\n",
    "            print(\"Custom Metric Value (Balanced Accuracy):\", custom_metric_value)\n",
    "        \n",
    "    def plot_training_history(self, history):\n",
    "        # Plot training & validation accuracy values\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_roc_curve(self):\n",
    "        y_pred_prob = self.model.predict(self.X_test).ravel()\n",
    "        fpr, tpr, thresholds = roc_curve(self.y_test, y_pred_prob)\n",
    "        auc_score = auc(fpr, tpr)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Preprocess input\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_reshaped = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_prob = self.model.predict(X_reshaped)\n",
    "        y_pred_rounded = np.round(y_pred_prob)\n",
    "        y_pred = y_pred_rounded.astype(int).ravel()\n",
    "        return y_pred\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        self.model.save(model_path)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        self.model = load_model(model_path)\n",
    "\n",
    "    def get_model_summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    def tune_hyperparameters(self, param_grid, cv=5, search_type='grid', n_iter=None, random_state=42):\n",
    "        input_shape = (1, self.X_train.shape[2])\n",
    "\n",
    "        # Wrap the model for use with scikit-learn\n",
    "        model = KerasClassifier(build_fn=lambda: create_lstm_model(input_shape), verbose=0)\n",
    "\n",
    "        if search_type == 'grid':\n",
    "            search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv)\n",
    "        elif search_type == 'random':\n",
    "            if n_iter is None:\n",
    "                raise ValueError(\"n_iter must be specified for random search.\")\n",
    "            search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=cv,\n",
    "                                        n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "        search_result = search.fit(self.X_train, self.y_train)\n",
    "\n",
    "        # Print the best score and best parameters\n",
    "        print(\"Best score: %f using %s\" % (search_result.best_score_, search_result.best_params_))\n",
    "        return search_result\n",
    "\n",
    "    def k_fold_cross_validation(self, n_splits=5, epochs=50, batch_size=32):\n",
    "        input_shape = (1, self.X_train.shape[2])\n",
    "\n",
    "        # Define a function to create the model with the proper input shape\n",
    "        def create_model():\n",
    "            return create_lstm_model(input_shape)\n",
    "\n",
    "        # Wrap the model for use with scikit-learn\n",
    "        model = KerasClassifier(build_fn=create_model, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        # Perform k-fold cross-validation\n",
    "        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        results = cross_val_score(model, self.X_train, self.y_train, cv=kfold)\n",
    "\n",
    "        # Print the mean and standard deviation of the cross-validation scores\n",
    "        print(\"Cross-Validation Accuracy: %.2f%% (%.2f%%)\" % (results.mean() * 100, results.std() * 100))\n",
    "\n",
    "    def model_to_json(self):\n",
    "        model_json = self.model.to_json()\n",
    "        return model_json\n",
    "\n",
    "    def json_to_model(self, model_json):\n",
    "        self.model = model_from_json(model_json)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_reshaped = np.reshape(X_scaled, (X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "        y_pred_prob = self.model.predict(X_reshaped)\n",
    "        return y_pred_prob\n",
    "\n",
    "    def train_val_split(self, val_size=0.1, random_state=42):\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            self.X_train, self.y_train, test_size=val_size, random_state=random_state)\n",
    "\n",
    "        # Reshape X_val using the same number of features as in X_train and X_test\n",
    "        self.X_val = np.reshape(self.X_val, (self.X_val.shape[0], 1, self.X_train.shape[2]))\n",
    "\n",
    "\n",
    "    def learning_rate_reduction(self, factor=0.1, patience=10, min_lr=1e-5):\n",
    "        return ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, min_lr=min_lr)\n",
    "\n",
    "    def train_model(self, epochs, batch_size, use_early_stopping=True, use_lr_reduction=True):\n",
    "        callbacks = []\n",
    "        if use_early_stopping:\n",
    "            callbacks.append(self.early_stopping())\n",
    "        if use_lr_reduction:\n",
    "            callbacks.append(self.learning_rate_reduction())\n",
    "\n",
    "        history = self.model.fit(self.X_train, self.y_train, epochs=epochs, batch_size=batch_size,\n",
    "                                 validation_data=(self.X_test, self.y_test), callbacks=callbacks)\n",
    "        return history\n",
    "    \n",
    "    def get_feature_importance(self, X, y, n_repeats=10, random_state=42):\n",
    "        # Wrap the predict_proba method for use with sklearn's permutation_importance function\n",
    "        def predict_proba_wrapped(X):\n",
    "            return self.predict_proba(X)\n",
    "\n",
    "        # Compute the permutation importance\n",
    "        result = permutation_importance(predict_proba_wrapped, X, y, n_repeats=n_repeats,\n",
    "                                        random_state=random_state, n_jobs=-1)\n",
    "\n",
    "        # Combine the feature importances and their names into a dataframe and sort by importance\n",
    "        feature_importance = pd.DataFrame({'feature': self.data.drop([\"churn\"], axis=1).columns,\n",
    "                                           'importance': result.importances_mean,\n",
    "                                           'std': result.importances_std})\n",
    "\n",
    "        feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "\n",
    "        return feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c64e8",
   "metadata": {},
   "source": [
    "# 2.4.2 <a id='2.4.2'> LSTM Training </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22880c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LSTMClassifier\n",
    "lstm_classifier = LSTMClassifier(df_train)\n",
    "\n",
    "# Preprocess the data\n",
    "lstm_classifier.preprocess_data()\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "lstm_classifier.train_val_split()\n",
    "\n",
    "# Build the model\n",
    "lstm_classifier.build_model()\n",
    "\n",
    "# Train the model with early stopping and learning rate reduction\n",
    "history = lstm_classifier.train_model(epochs=1000, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae384e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters\n",
    "param_grid = {\n",
    "    'epochs': [25, 50],\n",
    "    'batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "# grid_result = lstm_classifier.tune_hyperparameters(param_grid)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "# lstm_classifier.k_fold_cross_validation(n_splits=5, epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfad09",
   "metadata": {},
   "source": [
    "# 2.4.3 <a id='2.4.3'> Evaluate </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history\n",
    "lstm_classifier.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "lstm_classifier.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305911ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "lstm_classifier.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e27fe9",
   "metadata": {},
   "source": [
    "# 2.4.4 <a id='2.4.4'> LSTM Prediction </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e8caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = lstm_classifier.predict_proba(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54beef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lstm_classifier.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ac4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id':id_sub, 'churn':predicted})\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c33b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.churn.replace([0,1],['no','yes'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f271c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c5ccc",
   "metadata": {},
   "source": [
    "# 2.4.5 <a id='2.4.5'> Feature importance </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3de911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "# feature_importances = lstm_classifier.get_feature_importance(X_test, y_test)\n",
    "\n",
    "# Print the feature importances\n",
    "# print(\"Feature importances:\")\n",
    "# print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd66d1b",
   "metadata": {},
   "source": [
    "# 2.4.6 <a id='2.4.'> Save LSTM Model </a>\n",
    "[Table of contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09874d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "# classifier.save_model('lstm_model.h5')\n",
    "\n",
    "# Load the model from disk\n",
    "# classifier.load_model('lstm_model.h5')\n",
    "\n",
    "# Save the model architecture as a JSON string\n",
    "# model_json = classifier.model_to_json()\n",
    "\n",
    "# Load the model architecture from a JSON string\n",
    "# classifier.json_to_model(model_json)\n",
    "\n",
    "# Print the model architecture summary\n",
    "# classifier.get_model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab6f41",
   "metadata": {},
   "source": [
    "# Thanks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac809d1a",
   "metadata": {},
   "source": [
    "<img src=\"https://lh5.googleusercontent.com/9ROjm25aJ9h7n9dPSco1C0OOnEOdYXxO1omW_gAj6SUasnKVE3bqKMcLKzj0ZzLUUvBzrVHrnY2tYGLdJECV2X5_09Q1JAHv_zS3EvNGRNf6IoX9nEQkpPOa67hhBk6yQS53C1Hf\" width=\"400\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f39ac6b",
   "metadata": {},
   "source": [
    "<img src=\"https://www.vectorlogo.zone/logos/linkedin/linkedin-tile.svg\" align='left' alt=\"plotly\" width=\"60\" height=\"60\"/> </a><a> \n",
    "## [Msc. Diego Hurtado](https://www.linkedin.com/in/diegohurtadoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951f153b",
   "metadata": {},
   "source": [
    "<img src=\"https://www.vectorlogo.zone/logos/github/github-tile.svg\" align='left' alt=\"plotly\" width=\"60\" height=\"60\"/> </a><a> \n",
    "## [Msc. Diego Hurtado](https://github.com/DiegoHurtad0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be66a7",
   "metadata": {},
   "source": [
    "<img src=\"https://www.vectorlogo.zone/logos/medium/medium-tile.svg\" align='left' alt=\"plotly\" width=\"60\" height=\"60\"/> </a><a> \n",
    "## [Msc. Diego O‚ÄôHURTADO](https://medium.com/@diego.hurtado.olivares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a78f5",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/DiegoHurtad0/Covid-19-Dataset-Mexico/master/wave.svg\" width=\"900\" height=\"600\">\n",
    "\n",
    "## [Msc. Diego Hurtado](https://www.linkedin.com/in/diegohurtadoo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246b55e",
   "metadata": {},
   "source": [
    "## ‚ÄúWhen you are asked if you can do a job, tell ‚Äôem, ‚ÄòCertainly I can!‚Äô Then get busy and find out how to do it.‚Äù ‚Äî Theodore Roosevelt."
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1683404802537,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
